{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoaoAndryelDCOMP/Pok-mon-/blob/main/ESII.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHfO_HQP2zLF"
      },
      "outputs": [],
      "source": [
        "# 1. Instalar as bibliotecas necessárias\n",
        "\n",
        "!pip install transformers accelerate torch bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define o diretório de destino no Colab\n",
        "repo_dir = \"/content/DeepResearch\"\n",
        "\n",
        "# Verifica se a pasta já existe antes de clonar\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(f\"A clonar https://github.com/Alibaba-NLP/DeepResearch para {repo_dir}...\")\n",
        "\n",
        "    !git clone https://github.com/Alibaba-NLP/DeepResearch.git\n",
        "    print(\"Repositório clonado com sucesso.\")\n",
        "else:\n",
        "    print(f\"Repositório já existe em {repo_dir}.\")"
      ],
      "metadata": {
        "id": "Ak8X1FOa3Agb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "                                                 # Reinicie a sessão sempre que trocar o modelo\n",
        "# Nome do modelo que você quer\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"  # <--- Alterar pelo modelo escolhido:\n",
        "                                                 # deepseek-ai/deepseek-coder-6.7b-instruct\n",
        "print(f\"Carregando {model_id} em 4-bit...\")      # codellama/CodeLlama-7b-Instruct-hf\n",
        "                                                 # mistralai/Mistral-7B-Instruct-v0.3\n",
        "# --- Configuração de 4-bit  ---                 # microsoft/Phi-3-mini-128k-instruct\n",
        "bnb_config = BitsAndBytesConfig(                 # Qwen/Qwen2.5-7B-Instruct\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "\n",
        "# Carregar o tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Carregar o modelo aplicando a configuração de 4-bit\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,  # <-- Aplicando a configuração de 4-bit\n",
        "    device_map=\"auto\"                # \"auto\" coloca o modelo na GPU\n",
        ")\n",
        "\n",
        "print(\"----------------------------------------------------------\")\n",
        "print(f\"Modelo {model_id} carregado com sucesso em 4-bit!\")\n",
        "print(\"----------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "w92k2O_S3LvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# --- PASSO 1: COLETAR OS METADADOS ---\n",
        "\n",
        "repo_path = \"/content/DeepResearch\"\n",
        "print(f\"Coletando metadados do repositório em: {repo_path}\")\n",
        "\n",
        "# 1.1 Coletar Estrutura de Pastas\n",
        "folder_structure = \"\"\n",
        "for root, dirs, files in os.walk(repo_path, topdown=True):\n",
        "    # Limitar a profundidade da árvore (nível 0, 1 e 2)\n",
        "    depth = root.replace(repo_path, '').count(os.sep)\n",
        "    if depth > 2:\n",
        "        dirs[:] = [] # Não explorar mais fundo\n",
        "        continue\n",
        "\n",
        "    # Ignorar a pasta .git\n",
        "    if '.git' in dirs:\n",
        "        dirs.remove('.git')\n",
        "\n",
        "    # Formatar o nome da pasta\n",
        "    indent = \"  \" * depth\n",
        "    base_name = os.path.basename(root) if depth > 0 else 'DeepResearch'\n",
        "    folder_structure += f\"{indent}- {base_name}/\\n\"\n",
        "\n",
        "print(\"... Estrutura de Pastas coletada.\")\n",
        "\n",
        "# 1.2 Ler o README.md\n",
        "try:\n",
        "    with open(os.path.join(repo_path, \"README.md\"), 'r', encoding='utf-8') as f:\n",
        "        readme_content = f.read(3000) + \"\\n... (README truncado)\"\n",
        "    print(\"... README.md coletado.\")\n",
        "except Exception as e:\n",
        "    readme_content = f\"Erro ao ler README: {e}\"\n",
        "\n",
        "# 1.3 Ler o requirements.txt\n",
        "try:\n",
        "    with open(os.path.join(repo_path, \"requirements.txt\"), 'r', encoding='utf-8') as f:\n",
        "        requirements_content = f.read()\n",
        "    print(\"... requirements.txt coletado.\")\n",
        "except Exception as e:\n",
        "    requirements_content = f\"Erro ao ler requirements.txt: {e}\"\n",
        "\n",
        "# 1.4 Coletar os logs de commit\n",
        "try:\n",
        "    log_command = [\"git\", \"log\", \"--oneline\", \"-n\", \"20\"] # 20 commits mais recentes\n",
        "    result = subprocess.run(log_command, cwd=repo_path, capture_output=True, text=True, check=True, encoding='utf-8')\n",
        "    git_log_content = result.stdout\n",
        "    print(\"... Logs de Commit (últimos 20) coletados.\")\n",
        "except Exception as e:\n",
        "    git_log_content = f\"Erro ao ler logs do Git: {e}\"\n",
        "\n",
        "# 1.5 Ler o setup.py\n",
        "try:\n",
        "    with open(os.path.join(repo_path, \"setup.py\"), 'r', encoding='utf-8') as f:\n",
        "        setup_content = f.read(1000) + \"\\n... (setup.py truncado)\"\n",
        "    print(\"... setup.py coletado.\")\n",
        "except Exception as e:\n",
        "    setup_content = \"setup.py não encontrado ou erro.\"\n",
        "\n",
        "# 1.6 Ler o Ponto de Entrada da API\n",
        "api_main_path = \"WebAgent/WebSailor/main.py\"\n",
        "try:\n",
        "    with open(os.path.join(repo_path, api_main_path), 'r', encoding='utf-8') as f:\n",
        "        api_main_content = f.read(2000) + f\"\\n... ({api_main_path} truncado)\"\n",
        "    print(f\"... {api_main_path} coletado.\")\n",
        "except Exception as e:\n",
        "    api_main_content = f\"{api_main_path} não encontrado ou erro.\"\n",
        "\n",
        "\n",
        "# --- PASSO 2: CONSTRUIR O PROMPT ARQUITETURAL ---\n",
        "\n",
        "prompt_arquitetural = f\"\"\"\n",
        "[INST]\n",
        "Você é um Arquiteto de Software Sênior. Sua tarefa é analisar os seguintes metadados e **trechos de código** de um repositório para identificar os Padrões Arquiteturais.\n",
        "\n",
        "Preste muita atenção em como os arquivos de código (como o `main.py`) **importam e usam** outros módulos.\n",
        "\n",
        "---\n",
        "### DADOS DO REPOSITÓRIO PARA ANÁLISE ###\n",
        "\n",
        "#### 1. README.md (Truncado) ####\n",
        "{readme_content}\n",
        "\n",
        "#### 2. requirements.txt (Dependências) ####\n",
        "{requirements_content}\n",
        "\n",
        "#### 3. Estrutura de Pastas (Nível 2) ####\n",
        "{folder_structure}\n",
        "\n",
        "#### 4. Logs de Commit Recentes ####\n",
        "{git_log_content}\n",
        "\n",
        "#### 5. Conteúdo do setup.py (Define o Pacote) ####\n",
        "{setup_content}\n",
        "\n",
        "#### 6. Conteúdo do Ponto de Entrada da API ({api_main_path}) ####\n",
        "{api_main_content}\n",
        "\n",
        "---\n",
        "\n",
        "### RELATÓRIO DE HIPÓTESE ARQUITETURAL ###\n",
        "\n",
        "Baseado **apenas** nos dados acima, responda:\n",
        "\n",
        "Responda **precisamente** às seguintes perguntas, baseando-se **apenas** nas evidências fornecidas:\n",
        "\n",
        "1️⃣. *Qual é o Padrão de Ponto de Entrada?*\n",
        "    * Qual tecnologia principal é usada como interface do sistema?\n",
        "    * *Justifique* analisando o arquivo {api_main_path} e o requirements.txt.\n",
        "\n",
        "2️⃣. *Qual é o Padrão de Estrutura de Código?*\n",
        "    * *Justifique* sua escolha.\n",
        "\n",
        "3️⃣. *Qual é o Padrão de Implantação?*\n",
        "    * Ao analise o código do `{api_main_path} e {folder_structure}\n",
        "\n",
        "\n",
        "4️⃣. *Resumo da Arquitetura:*\n",
        "    * Combine suas três respostas acima em uma descrição coesa da arquitetura geral.\n",
        "\n",
        "\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nPrompt de análise arquitetural construído e pronto para envio.\")\n",
        "\n",
        "\n",
        "# --- PASSO 3: CHAMAR O LLM ---\n",
        "\n",
        "print(\"Enviando prompt de análise arquitetural para o LLM...\")\n",
        "\n",
        "try:\n",
        "\n",
        "    inputs = tokenizer(prompt_arquitetural, return_tensors=\"pt\", truncation=True, max_length=8192).to(\"cuda\") # 8k para CodeLlama/Mistral\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2024, # Espaço para o relatório\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.7 # Temperatura mais baixa ajuda a evitar alucinações\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    print(\"\\n\\n--- RELATÓRIO ARQUITETURAL DO LLM ---\")\n",
        "    print(response)\n",
        "\n",
        "except NameError as ne:\n",
        "    print(f\"\\nErro de Nome: {ne}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nErro ao rodar a análise: {e}\")"
      ],
      "metadata": {
        "id": "tjT7j1ho432j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}